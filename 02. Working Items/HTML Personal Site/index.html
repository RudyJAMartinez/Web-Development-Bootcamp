<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rudy Martinez</title>
</head>

<body>
    <table cellspacing="20">
        <tr>
            <td>
                <!--Image: Include Personal Image Using Image Tag-->
                <img src="Images/Rudy Martinez Photo.png" alt="Rudy Martinez Profile Picture" width="250" height="250">
            </td>
            <td>
                <!--Title: Includes Name and Brief Description-->
                <h1> Rudy Martinez</h1>
                <h3> Capital Markets Analyst at PenFed | <em></em>Data Analytics Graduate Student at UTSA</em></h3>
                <p><em><a href="contact_info.html"> Contact Information</a></em></p>
            </td>
        </tr>
    </table>
    <h3>At the core, a servant leader striving to create value for others everyday. Passionate about influencing the
        business through data driven stories. Utilizes <strong>Python, SQL, R, and Tableau</strong> for advanced
        querying, data cleaning, analysis, and visualizations. Experienced in interpreting data and communicating
        insights to drive decision making.</h3>
    <hr size="2" , noshade>

    <!--Education: Includes Information on Education-->
    <h2>Education</h2>
    <table>
        <thead align="left">
            <tr>
                <th>University</th>
                <th>   </th>
                <th>Degree</th>
                <th>GPA</th>
            </tr>
        </thead>
        <tr>
            <td>University of Texas at San Antonio</td>
            <td>   </td>
            <td>Master of Science in Data Analytics</td>
            <td>4.00</td>
        </tr>
        <tr>
            <td>St. Mary's University</td>
            <td>   </td>
            <td>BBA in Finance & Risk Management</td>
            <td>3.91</td>
        </tr>
    </table>
    <hr size="2" , noshade>

    <!--Technical Skills: Includes Information on Technical Skills-->
    <h2>Technical Skills</h2>
    <table>
        <tr>
            <td><strong>Languages</strong></td>
            <td>Python, SQL, R, HTML</td>
            <td> </td>

        </tr>
        <tr>
            <td><strong>Tools & Applications</strong></td>
            <td>Data Science Tools (<em>Pandas, Numpy, scikit-learn, tensorflow</em>), Web/PDF Scraping
                (<em>BeautifulSoup,
                    tika, regex</em>), Visualization (<em>Tableau, Matplotlib, ggplot2</em>), Jupyter Notebooks, RShiny,
                Jira,
                Excel (<em>Pivot Tables, Vlookup, Index Match</em>)</td>
            <td> </td>
        </tr>
        <tr>
            <td><strong>Relevant Coursework</strong></td>
            <td>Algorithms I/II (<em>R</em>), Data Analytics Applications (<em>Python</em>), Deep Learning on Cloud
                Platforms (<em>Python</em>),
                Visualizations and Communications (<em>Tableau & R</em>)</td>
            <td> </td>
        </tr>
    </table>
    <hr size="2" , noshade>

    <!--Project Experience: Includes Information on Project Experience-->
    <h2>Project Experience</h2>
    <p><strong><a href="https://www.youtube.com/watch?v=kA-B8Ax1iEM">Global Terrorism Database (GTD) Predictive
                Analysis</a></strong></p>
    <ul>
        <li>Built classification models (Decision Tree, K Nearest Neighbors, SGD) that determine which active terrorist
            organization is responsible for attacks based on attribute fields available in the GTD</li>
        <li>Performed data cleaning and preprocessing using Python pandas, numpy, and sklearn to enhance data
            cleanliness and optimize feature selection ahead of model building</li>
        <li>Deemed Decision Tree the best classifier due to its 73% accuracy rate in classifying the perpetrator group
        </li>
    </ul>

    <p><strong><a href="https://www.youtube.com/watch?v=K4bufsyR8l4">Python Automated Financial & Sentiment Analysis</a>
    </p></strong>
    <ul>
        <li>Developed a Python web scraping program that scrapes S&P 500 companies from Wikipedia and scrapes Yahoo
            Finance
            key statistics, financial statements and stock price history </li>
        <li>Coded a multi-layer stock screen to determine the company's financial strength (analysis via the Piotroski
            F-Score)</li>
        <li>Conducted sentiment analysis on scraped news article headings, resulting in a compound score that indicates
            positive,
            negative, or neutral sentiment</li>
    </ul>

    <p><strong><a href="https://www.youtube.com/watch?v=K4bufsyR8l4"> Real Estate Market Analysis</a></strong></p>
    <ul>
        <li>Utilized the Realtor.com Inventory and Market Hotness dataset to visualize trends in the San Antonio real
            estate market in Tableau</li>
        <li>Provided analysis on real estate metrics that enables consumers to make more informed buy and sell decisions
            in the wake of San Antonio
            real estate markets trends</li>
    </ul>

    <hr size="2" , noshade>

    <!--Experience: Includes Full Time and Internship Positions-->
    <h2>Experience</h2>
    <p><strong>Pentagon Federal Credit Union</strong> | Capital Markets Analyst</p>
    <p><em></e>September 2021 - Present</p></em>
    <ul>
        <li>Wrote Python script to query raw auto loan data and extract an eligible pool of loans (subset) by
            implementing securitization pooling criteria. The script incorporates the use of Python pandas and
            numpy for data handling and cleaning, and it integrates the use of SQL to perform the query on the
            cleaned data set.</li>
        <li>Developed Python script to read in eligible loans for securitization and automatically build
            stratification tables by Geography, Original Term to Maturity, Remaining Term to Maturity,
            FICO Score, APR, and Current Balance. The script incorporates the use of pandas and numpy for data
            handling and cleaning, and it performs automatic calculations to determine the stratification table
            ranges.</li>
        <li>Created Python script (File Forager) to navigate to a Share Drive and export a full list of all file
            names and paths in the specified directory. The script then searches for loan number matches in the
            aggregated list of file paths, exports those matches to a csv, and copies the matched files into a
            specified destination folder. Additionally, an automated checklist is produced to aid the user in
            determining the count of matches per loan across various file categories. The script incorporates the
            use of pathlib, datetime, os, pandas, and re libraries.</li>
        <li>Wrote Python script that reads in data from two sources, runs a comparison between the files, and
            identifies instances in which there is a data mismatch (discrepancies). The code then exports csv
            documents for each field name, detailing the specific rows of data where there are discrepancies.
            The script incorporates the use of pandas and numpy as well as user-developed functions to filter
            the data.</li>
    </ul>

    <p><strong>USAA</strong> | Capital Markets Analyst III & II</p>
    <p><em>May 2020 - September 2021</p></em>
    <ul>
        <li>Led the issuance and execution of $400M in catastrophe bonds on behalf of P&C through Residential
            Reinsurance 2021-I, serving as a reliable business partner in a high-urgency environment</li>
        <li>Prepared Residential Reinsurance Transaction Offering Materials via Wdesk by reaching out to appropriate
            stakeholders, hosting meetings to discuss discrepancies and finalizing changes through a certification
            process</li>
        <li>Served as Project Manager within the LIBOR Program Management Office (PMO), coordinating with cross
            functional partners in Real Estate Servicing, Mortgage Trading, Investments, Modeling, Legal, and Compliance
        </li>
        <li>Created a Python program that scrapes 500+ pages of PDF legal documentation using the tika package. The
            program pinpoints areas in the PDF with a variety of financial terms and values that the Capital Markets
            team utilizes. Using regular expressions and the Pandas Python package, the program consolidates and
            reformats the captured data and exports the information to CSV. Thus, ensuring additional operational
            efficiency for the team</li>
        <li>Created a Python program that web scrapes the United States Department of Treasury site for various tenors
            of Treasury Yields. The program identifies the rates in the html code, consolidates the rates, and exports
            the values to CSV. These rates are then use to create visualizations for presentation materials</li>
        <li>Created a Python program that web scrapes the Alternative Reference Rates Committee (ARRC) site, identifying
            the links where committee meeting minutes are stored and exporting the minutes as PDF files to my local
            machine</li>
        <li>Collected and cleaned data from Enterprise partners to refresh LIBOR monthly exposure materials and ensure
            effective reporting to internal and external partners on a regular cadence</li>
        <li>Served as Project Manager within the LIBOR Program Management Office (PMO), cutting costs by $500,000 and
            communicating with cross functional partners in Real Estate Servicing, Mortgage Trading, Investments,
            Modeling, Legal, and Compliance</li>
        <li>Worked with LIBOR Transition stakeholders to develop LIBOR Transition White Papers, Presentation materials,
            and regulator response documents</li>
        <li>Created a standardized certification process for LIBOR Transition Executive Management approval prior to
            releasing documentation to regulators</li>
        <li>Took responsibility for the LIBOR Transition data collection process by training appropriate stakeholders on
            how to use Wdesk for data uploading to preserve an audit trail</li>
        <li>Aided in the issuance and execution of $400 Million in Catastrophe Bonds on behalf of P&C through
            Residential Reinsurance 2020-II, serving as a reliable analyst in a high-urgency environment</li>
        <li>Aided in the payment of $20+ million in Premiums to Residential Reinsurance investors as part of the Q4'2020
            quarterly premium payment process by performing the PeopleSoft origin upload, attaching invoices to
            PeopleSoft, and pushing the invoices for approval.</li>
        <li>Revamped the Catastrophe Bond Quarterly Premium Payment schedule provided to Accounting / Tax, offering a
            simplified view of premium payments that enable them to more efficiently execute journal entries in the
            reinsurance accounting system</li>
    </ul>

    <p><strong>Rackspace Technologies, Inc.</strong> | Financial Systems Analyst, Intern I & II</p>
    <p><em>May 2017 - May 2020</p></em>
    <ul>
        <li>Managed 4 system databases, processing SNOW tickets for users located across the globe in different time
            zones</li>
        <li>Assisted team members with documentation of processes, procedures, and with UAT for system configuration
            changes and new functionality</li>
        <li>Assisted users with production support items for Oracle, Hyperion, Blackline, and other systems used by the
            CFO group</li>
        <li>Performed chart of accounts setups & maintenance in Oracle</li>
        <li>Tracked, logged, & executed production support tickets in SNOW – Service NOW</li>
        <li>Performed ongoing maintenance of Hyperion audit and error logs; Performed updates of team’s internal website
        </li>
        <li>Maintained Hyperion and Blackline access (provisioning or removing access)</li>
        <li>Established professional relationships with users to assist with production support items for Oracle,
            Hyperion,
            Blackline, and other financial systems used by the CFO group</li>
        <li>Performed chart of accounts setups in Oracle and metadata changes in Hyperion to promote optimal financial
            evaluation</li>
    </ul>

    <p><strong>USAA</strong> | Financial Analyst III, Intern</p>
    <p><em>May 2019 - August 2019</p></em>
    <ul>
        <li>Strengthened compliance and risk management practices by ensuring UDA processes and documenting risks and
            controls within the IPF Finance team</li>
        <li>Performed Discipline Execution for forecast consolidations, creating SIPOCs, Process Maps, and Controls that
            advance the company’s journey to
            enterprise-wide compliance</li>
        <li>Developed an automated process to capture user access and fortify share drive security, leveraging
            SharePoint for inputs and control evidence</li>
        <li>Assisted with peer reviewing monthly analysis and creating MBM package and associated reports for the CFO
        </li>
    </ul>

    <p><strong>USAA</strong> | Financial Analyst III, Intern</p>
    <p><em>May 2018 - August 2018</p></em>
    <ul>
        <li>Automated the rolling of monthly forecast reports, improving forecast execution efficiency, minimizing room
            for human error, and prepping for the
            integration of financial assumptions</li>
        <li>Executed ad-hoc analysis via Hyperion Essbase, generating grids to pull relevant data and analyze average
            written premiums, product volumes, loss
            trends (pure premium), and various other pertinent financial items</li>
        <li>Performed analysis of Actuals, Rate, Trend, and Volumes changes to detail overall impact to revenue by
            product</li>
        <li>Facilitated the execution process for delivering financial analysis and results to senior P&C Management
        </li>
    </ul>

    <hr size="2" , noshade>

</body>

</html>